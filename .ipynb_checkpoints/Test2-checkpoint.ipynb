{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315bd33-e66b-4554-b6ec-9492eec33383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Imports & Paths & Constants\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "from libpysal.weights import Queen\n",
    "\n",
    "## 1.1 Paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR  = DATA_DIR / \"raw\"\n",
    "GEO_DIR  = DATA_DIR / \"geo\"\n",
    "TABLE_DIR = DATA_DIR / \"table\"\n",
    "\n",
    "for d in [DATA_DIR, RAW_DIR, GEO_DIR, TABLE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## 1.2 ORCA Configs\n",
    "CALENDAR_YMD = \"20240614\"\n",
    "LISTINGS_YMD = \"20250615\"\n",
    "CITY = \"London\"\n",
    "\n",
    "HOST = \"https://orca.casa.ucl.ac.uk\"\n",
    "ORCA_PATH = \"~jreades/data\"\n",
    "\n",
    "CALENDAR_FILE = f\"{CALENDAR_YMD}-{CITY}-calendar.csv.gz\"\n",
    "LISTINGS_FILE = f\"{LISTINGS_YMD}-{CITY}-listings.csv.gz\"\n",
    "\n",
    "## 1.3 Thresholds / Constants\n",
    "VIOLATION_THRESHOLD        = 90\n",
    "COMMERCIAL_AVAIL_THRESHOLD = 60\n",
    "HOTEL_LIKE_OCC_THRESHOLD   = 180\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81f27d-d7d5-438c-b86f-fee7b9e3f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Cache/Download Helpers & Download ORCA & Download Borough polygons\n",
    "## 2.1 Cache\n",
    "def check_cache(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(src: str, dst_dir: Path, min_size: int = 1000) -> Path:\n",
    "        fn = Path(src.split(\"?\")[0]).name\n",
    "        dst = dst_dir / fn\n",
    "        if dst.is_file() and dst.stat().st_size > min_size:\n",
    "            print(f\"+ cached: {dst}\")\n",
    "            return dst\n",
    "        print(f\"+ downloading: {dst}\")\n",
    "        return f(src, dst)\n",
    "    return wrapper\n",
    "\n",
    "@check_cache\n",
    "def cache_data(src: str, dst: Path) -> Path:\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    r = requests.get(src)\n",
    "    r.raise_for_status()\n",
    "    dst.write_bytes(r.content)\n",
    "    print(\"+ done\")\n",
    "    return dst.resolve()\n",
    "\n",
    "## 2.2 Download ORCA Listing/Calendar\n",
    "listings_url = f\"{HOST}/{ORCA_PATH}/{LISTINGS_FILE}\"\n",
    "calendar_url = f\"{HOST}/{ORCA_PATH}/{CALENDAR_FILE}\"\n",
    "\n",
    "listings_path = cache_data(listings_url, RAW_DIR)\n",
    "calendar_path = cache_data(calendar_url, RAW_DIR)\n",
    "\n",
    "## 2.3 Download Borough Polygons (gpkg)\n",
    "borough_url = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Boroughs.gpkg\"\n",
    "borough_path = GEO_DIR / \"Boroughs.gpkg\"\n",
    "borough_path = cache_data(borough_url, GEO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13471b84-20b8-4614-baa8-1576c3f645d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Core Wrangling: read listings & stream calendar & merge & flags\n",
    "CHUNK_SIZE = 200_000\n",
    "\n",
    "def load_listings(path: Path) -> pd.DataFrame:\n",
    "    cols_in_file = pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "    desired_cols = [\n",
    "        \"id\", \"host_id\", \"room_type\",\n",
    "        \"neighbourhood_cleansed\", \"neighbourhood_group_cleansed\",\n",
    "        \"latitude\", \"longitude\",\n",
    "        \"number_of_reviews\", \"price\",\n",
    "    ]\n",
    "    usecols = [c for c in desired_cols if c in cols_in_file]\n",
    "    df = pd.read_csv(path, usecols=usecols, low_memory=False)\n",
    "\n",
    "    # clean price if present\n",
    "    if \"price\" in df.columns:\n",
    "        price_str = df[\"price\"].astype(str).str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "        df[\"price_clean\"] = pd.to_numeric(price_str, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def summarise_calendar_streaming(\n",
    "    path: Path,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    start_date: str = \"2024-06-14\",\n",
    "    end_date: str   = \"2025-06-14\",\n",
    ") -> pd.DataFrame:\n",
    "    agg = {}  # {listing_id: [occupied_nights, total_nights]}\n",
    "\n",
    "    for chunk in pd.read_csv(\n",
    "        path,\n",
    "        chunksize=chunk_size,\n",
    "        usecols=[\"listing_id\", \"available\", \"date\"],\n",
    "        low_memory=False\n",
    "    ):\n",
    "        chunk[\"date\"] = pd.to_datetime(chunk[\"date\"], errors=\"coerce\")\n",
    "        chunk = chunk.dropna(subset=[\"date\"])\n",
    "\n",
    "        mask = (chunk[\"date\"] >= start_date) & (chunk[\"date\"] < end_date)\n",
    "        chunk = chunk.loc[mask].copy()\n",
    "\n",
    "        # InsideAirbnb: available = \"t\"/\"f\" sometimes; normalize\n",
    "        av = chunk[\"available\"].astype(str).str.lower()\n",
    "        chunk[\"is_available\"] = av.isin([\"t\", \"true\", \"1\", \"yes\"])\n",
    "\n",
    "        for lid, g in chunk.groupby(\"listing_id\"):\n",
    "            total = len(g)\n",
    "            occ = int((~g[\"is_available\"]).sum())  # occupied nights\n",
    "            if lid not in agg:\n",
    "                agg[lid] = [0, 0]\n",
    "            agg[lid][0] += occ\n",
    "            agg[lid][1] += total\n",
    "\n",
    "    summary = (\n",
    "        pd.DataFrame.from_dict(agg, orient=\"index\", columns=[\"occupied_nights\", \"total_nights\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"listing_id\"})\n",
    "    )\n",
    "    summary[\"available_nights\"] = summary[\"total_nights\"] - summary[\"occupied_nights\"]\n",
    "    summary[\"occupancy_rate\"] = summary[\"occupied_nights\"] / summary[\"total_nights\"]\n",
    "    return summary\n",
    "\n",
    "def merge_calendar_listings(occ_summary: pd.DataFrame, listings: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols_to_keep = [\n",
    "        \"id\", \"host_id\", \"room_type\",\n",
    "        \"neighbourhood_cleansed\", \"neighbourhood_group_cleansed\",\n",
    "        \"latitude\", \"longitude\",\n",
    "        \"number_of_reviews\", \"price_clean\",\n",
    "    ]\n",
    "    existing_cols = [c for c in cols_to_keep if c in listings.columns]\n",
    "\n",
    "    merged = occ_summary.merge(\n",
    "        listings[existing_cols].drop_duplicates(subset=\"id\"),\n",
    "        left_on=\"listing_id\",\n",
    "        right_on=\"id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "def add_core_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # entire home flag\n",
    "    if \"room_type\" in out.columns:\n",
    "        out[\"is_entire_home\"] = out[\"room_type\"].astype(str).str.contains(\"entire\", case=False, na=False)\n",
    "    else:\n",
    "        out[\"is_entire_home\"] = False\n",
    "\n",
    "    # 90-day rule violation (entire homes only)\n",
    "    out[\"violates_90day\"] = (out[\"is_entire_home\"]) & (out[\"occupied_nights\"] > VIOLATION_THRESHOLD)\n",
    "\n",
    "    # commercial STR (your logic: available nights > threshold)\n",
    "    out[\"commercial_STR\"] = (out[\"is_entire_home\"]) & (out[\"available_nights\"] > COMMERCIAL_AVAIL_THRESHOLD)\n",
    "\n",
    "    # hotel-like (your logic: occupied nights > threshold)\n",
    "    out[\"hotel_like_STR\"] = (out[\"is_entire_home\"]) & (out[\"occupied_nights\"] > HOTEL_LIKE_OCC_THRESHOLD)\n",
    "\n",
    "    return out\n",
    "\n",
    "def prepare_merged(calendar_path: Path, listings_path: Path) -> pd.DataFrame:\n",
    "    print(\"=== Loading calendar (streaming) ===\")\n",
    "    occ_summary = summarise_calendar_streaming(calendar_path)\n",
    "    print(\"calendar summary:\", occ_summary.shape)\n",
    "\n",
    "    print(\"=== Loading listings ===\")\n",
    "    listings = load_listings(listings_path)\n",
    "    print(\"listings:\", listings.shape)\n",
    "\n",
    "    print(\"=== Merging + flags ===\")\n",
    "    merged = merge_calendar_listings(occ_summary, listings)\n",
    "    merged = add_core_flags(merged)\n",
    "\n",
    "    print(\"merged:\", merged.shape)\n",
    "    return merged\n",
    "\n",
    "merged = prepare_merged(calendar_path=calendar_path, listings_path=listings_path)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0dadd-40a4-40bc-b03e-ffde652d74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Q1.1 Policy (90-day rule)\n",
    "## 4.1 Function Definitions\n",
    "def _unique_entire_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[df[\"is_entire_home\"]].drop_duplicates(subset=\"listing_id\").copy()\n",
    "\n",
    "def citywide_violation_stats(df: pd.DataFrame) -> dict:\n",
    "    entire = _unique_entire_listings(df)\n",
    "    total_entire = entire[\"listing_id\"].nunique()\n",
    "\n",
    "    n_viol = int(entire[\"violates_90day\"].sum())\n",
    "    share_viol = n_viol / total_entire if total_entire > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"total_entire_homes\": total_entire,\n",
    "        \"n_violations\": n_viol,\n",
    "        \"share_violations\": share_viol,\n",
    "    }\n",
    "\n",
    "def violation_by_neighbourhood(\n",
    "    df: pd.DataFrame,\n",
    "    borough_col: str = \"neighbourhood_cleansed\"\n",
    ") -> pd.DataFrame:\n",
    "    entire = _unique_entire_listings(df).dropna(subset=[borough_col]).copy()\n",
    "\n",
    "    out = (\n",
    "        entire.groupby(borough_col, as_index=False)\n",
    "        .agg(\n",
    "            n_entire=(\"listing_id\", \"nunique\"),\n",
    "            n_violations=(\"violates_90day\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    out[\"share_violations\"] = out[\"n_violations\"] / out[\"n_entire\"]\n",
    "    out = out.rename(columns={borough_col: \"borough\"})\n",
    "    return out\n",
    "\n",
    "def plot_occupied_nights_hist(df: pd.DataFrame) -> None:\n",
    "    entire = _unique_entire_listings(df)\n",
    "    x = entire[\"occupied_nights\"].dropna()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(x, bins=50)\n",
    "    plt.axvline(VIOLATION_THRESHOLD, linestyle=\"--\")\n",
    "    plt.xlabel(\"Occupied nights (entire homes)\")\n",
    "    plt.ylabel(\"Number of listings\")\n",
    "    plt.title(\"Figure-1 Distribution of Occupied Nights (Entire Homes)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_violation_areas(neigh_stats: pd.DataFrame, top_n: int = 20) -> None:\n",
    "    df = neigh_stats.sort_values(\"share_violations\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(df[\"borough\"], df[\"share_violations\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Share of entire homes violating 90-day rule\")\n",
    "    plt.title(f\"Figure-2 Top {top_n} Areas by Violation Share\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def commercial_by_neighbourhood(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = (\n",
    "        df[df[\"is_entire_home\"]]\n",
    "        .drop_duplicates(subset=\"listing_id\")\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    out = (\n",
    "        d.groupby(\"neighbourhood_cleansed\", as_index=False)\n",
    "        .agg(\n",
    "            n_entire=(\"listing_id\", \"nunique\"),\n",
    "            n_commercial=(\"commercial_STR\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    out[\"commercial_share\"] = out[\"n_commercial\"] / out[\"n_entire\"]\n",
    "    out = out.rename(columns={\"neighbourhood_cleansed\": \"borough\"})\n",
    "    return out\n",
    "    \n",
    "def build_neighbourhood_policy_commercial_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a borough-level table with:\n",
    "      - share_violations (90-day rule)\n",
    "      - commercial_share (commercial STR)\n",
    "    \"\"\"\n",
    "    neigh_policy = violation_by_neighbourhood(df)         # borough + share_violations\n",
    "    neigh_comm   = commercial_by_neighbourhood(df)        # borough + commercial_share\n",
    "\n",
    "    neigh_stats = (\n",
    "        neigh_policy\n",
    "        .merge(neigh_comm[[\"borough\", \"commercial_share\"]], on=\"borough\", how=\"inner\")\n",
    "        .dropna(subset=[\"share_violations\", \"commercial_share\"])\n",
    "    )\n",
    "    return neigh_stats\n",
    "    \n",
    "def plot_share_commercial_vs_violations(neigh_stats: pd.DataFrame) -> None:\n",
    "    df = neigh_stats.dropna(subset=[\"share_violations\", \"commercial_share\", \"borough\"]).copy()\n",
    "    if df.empty:\n",
    "        print(\"No valid boroughs for Figure 3, skipping plot.\")\n",
    "        return\n",
    "\n",
    "    df[\"borough_label\"] = df[\"borough\"].astype(str).str.strip().str.title()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.scatter(df[\"share_violations\"], df[\"commercial_share\"], s=60, alpha=0.8)\n",
    "    \n",
    "    dx, dy = 0.002, 0.002\n",
    "    for _, r in df.iterrows():\n",
    "        plt.text(\n",
    "            r[\"share_violations\"] + dx,\n",
    "            r[\"commercial_share\"] + dy,\n",
    "            r[\"borough_label\"],\n",
    "            fontsize=9\n",
    "        )\n",
    "    plt.xlabel(\"Share of 90-day violations (entire homes)\")\n",
    "    plt.ylabel(\"Share of commercial STR (entire homes)\")\n",
    "    plt.title(\"Figure-3 Commercial STR vs 90-day Violations by Borough\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b510e0-6232-4e34-810e-1d7b75df3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2 Citywide 90-day rule stats + Figure 1 (toggleable)\n",
    "RUN_CITYWIDE = True  \n",
    "\n",
    "if RUN_CITYWIDE:\n",
    "    city = citywide_violation_stats(merged)\n",
    "\n",
    "    print(\"=== Citywide 90-day rule stats ===\")\n",
    "    print(city)\n",
    "\n",
    "    # Figure 1\n",
    "    plot_occupied_nights_hist(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d290e-3130-46de-a08c-f6db654d70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.3 Neighbourhood (borough) table + Figure 2 (toggleable)\n",
    "RUN_NEIGH = True           \n",
    "SHOW_NEIGH_TABLE = True   \n",
    "SHOW_NEIGH_PLOT  = True   \n",
    "\n",
    "TOP_N = 20\n",
    "\n",
    "if RUN_NEIGH:\n",
    "    neigh = violation_by_neighbourhood(merged)\n",
    "\n",
    "    print(\"\\n=== Neighbourhood (borough) violation table (top 10 by share) ===\")\n",
    "    print(neigh.sort_values(\"share_violations\", ascending=False).head(10))\n",
    "\n",
    "    # Figure 2\n",
    "    plot_top_violation_areas(neigh, top_n=TOP_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf79ab4-05d2-4173-8b23-659130551fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Policy Figure 3: Commercial STR vs 90-day violations (toggleable)\n",
    "RUN_FIG3 = True\n",
    "SHOW_FIG3_PLOT = True\n",
    "\n",
    "if RUN_FIG3:\n",
    "    neigh_stats = build_neighbourhood_policy_commercial_stats(merged)\n",
    "\n",
    "    if SHOW_FIG3_PLOT:\n",
    "        print(\"\\n=== Policy Figure 3: Commercial STR vs 90-day violations ===\")\n",
    "        plot_share_commercial_vs_violations(neigh_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96615ea3-2b66-4fb4-8ab2-62167e9a38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Q1.2 Commercialisation\n",
    "## 5.1 Function Definitions\n",
    "def _unique_listings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop_duplicates(subset=\"listing_id\").copy()\n",
    "\n",
    "def compute_entire_home_stats(df: pd.DataFrame) -> dict:\n",
    "    d = _unique_listings(df)\n",
    "\n",
    "    total_listings = d[\"listing_id\"].nunique()\n",
    "    entire = d[d[\"is_entire_home\"]]\n",
    "    n_entire = entire[\"listing_id\"].nunique()\n",
    "    share_entire = n_entire / total_listings if total_listings > 0 else np.nan\n",
    "\n",
    "    n_commercial = int(entire[\"commercial_STR\"].sum())\n",
    "    share_commercial = n_commercial / n_entire if n_entire > 0 else np.nan\n",
    "\n",
    "    n_hotel_like = int(entire[\"hotel_like_STR\"].sum())\n",
    "    share_hotel_like = n_hotel_like / n_entire if n_entire > 0 else np.nan\n",
    "\n",
    "    n_legal_but_commercial = int(\n",
    "        ((~entire[\"violates_90day\"]) & (entire[\"commercial_STR\"])).sum()\n",
    "    )\n",
    "    share_legal_but_comm = n_legal_but_commercial / n_entire if n_entire > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"total_listings\": total_listings,\n",
    "        \"n_entire\": n_entire,\n",
    "        \"share_entire\": share_entire,\n",
    "        \"n_commercial_entire\": n_commercial,\n",
    "        \"share_commercial_entire\": share_commercial,\n",
    "        \"n_hotel_like_entire\": n_hotel_like,\n",
    "        \"share_hotel_like_entire\": share_hotel_like,\n",
    "        \"n_legal_but_commercial_entire\": n_legal_but_commercial,\n",
    "        \"share_legal_but_commercial_entire\": share_legal_but_comm,\n",
    "    }\n",
    "\n",
    "def host_structure(df: pd.DataFrame) -> dict:\n",
    "    d = _unique_listings(df)\n",
    "    host_counts = d.groupby(\"host_id\")[\"listing_id\"].nunique()\n",
    "    return {\"host_counts\": host_counts}\n",
    "\n",
    "def plot_host_distribution(host_structure_stats: dict, max_listings: int = 12) -> None:\n",
    "    host_counts: pd.Series = host_structure_stats[\"host_counts\"]\n",
    "    vc = host_counts.value_counts().sort_index()\n",
    "    vc = vc[vc.index <= max_listings]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(vc.index.astype(int), vc.values)\n",
    "    plt.xlabel(\"Listings per host\")\n",
    "    plt.ylabel(\"Number of hosts\")\n",
    "    plt.title(\"Figure-4 Host Distribution (Capped)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_commercial_composition(stats: dict) -> None:\n",
    "    labels = [\n",
    "        \"Commercial STR\",\n",
    "        \"Hotel-like STR\",\n",
    "        \"Legal but commercial\",\n",
    "    ]\n",
    "    values = [\n",
    "        stats[\"share_commercial_entire\"],\n",
    "        stats[\"share_hotel_like_entire\"],\n",
    "        stats[\"share_legal_but_commercial_entire\"],\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylabel(\"Share of entire-home listings\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Figure-5 Commercialisation Composition (Entire Homes)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3f950-d3f4-41a2-99ed-74f77a104f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.2 Compute Stats Only\n",
    "RUN_COMMERCIAL = True\n",
    "SHOW_COMM_STATS = False\n",
    "\n",
    "if RUN_COMMERCIAL:\n",
    "    stats = compute_entire_home_stats(merged)\n",
    "    hosts = host_structure(merged)\n",
    "\n",
    "    if SHOW_COMM_STATS:\n",
    "        print(\"=== Commercialisation summary (entire homes) ===\")\n",
    "        for k, v in stats.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    results_commercial = {\n",
    "        \"stats\": stats,\n",
    "        \"hosts\": hosts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671ab17-d3d6-4375-b735-fbef2791e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.3 Commercialisation Figure 1: Host distribution\n",
    "SHOW_HOST_PLOT = True   \n",
    "\n",
    "if SHOW_HOST_PLOT:\n",
    "    plot_host_distribution(results_commercial[\"hosts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56294ac3-83cd-43b9-b249-3a3b8a0ea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.4 Commercialisation Figure 2: Commercial composition\n",
    "SHOW_COMM_COMP_PLOT = True   \n",
    "\n",
    "if SHOW_COMM_COMP_PLOT:\n",
    "    plot_commercial_composition(results_commercial[\"stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52053b67-3771-41f3-99ca-1bf733db8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Spatial Join & Build Borough-level Tables \n",
    "## 6.1 Read Brough Polygons \n",
    "boros = gpd.read_file(borough_path)\n",
    "boros[\"borough\"] = boros[\"NAME\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "## 6.2 Points + Spatial join (new object; boros unchanged)\n",
    "pts = gpd.GeoDataFrame(\n",
    "    merged.copy(),\n",
    "    geometry=gpd.points_from_xy(merged[\"longitude\"], merged[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(boros.crs)\n",
    "\n",
    "pts_boro = (\n",
    "    gpd.sjoin(pts, boros[[\"borough\", \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    .dropna(subset=[\"borough\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "## 6.3 Spatial Map 1: STR density per 1000 dwellings \n",
    "STOCK_CSV_PATH = TABLE_DIR / \"social-landlord-housing-stock-borough.csv\"\n",
    "stock = pd.read_csv(STOCK_CSV_PATH)\n",
    "\n",
    "housing = stock.copy()\n",
    "housing[\"borough\"] = housing[\"Area\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "dwell_col = \"Number of self-contained units or bedspaces-2024\"\n",
    "housing[\"dwellings\"] = (\n",
    "    housing[dwell_col].astype(str).str.replace(\",\", \"\", regex=False)\n",
    ")\n",
    "housing[\"dwellings\"] = pd.to_numeric(housing[\"dwellings\"], errors=\"coerce\")\n",
    "housing = housing[[\"borough\", \"dwellings\"]]\n",
    "\n",
    "borough_density = (\n",
    "    pts_boro.groupby(\"borough\", as_index=False)\n",
    "    .agg(n_listings=(\"listing_id\", \"nunique\"))\n",
    ")\n",
    "density_df = borough_density.merge(housing, on=\"borough\", how=\"left\")\n",
    "density_df[\"str_density_per_1000\"] = density_df[\"n_listings\"] / density_df[\"dwellings\"] * 1000\n",
    "\n",
    "boros_density = boros.merge(density_df[[\"borough\", \"str_density_per_1000\"]], on=\"borough\", how=\"left\")\n",
    "\n",
    "## 6.4 Spatial Map 2: Commercial STR share\n",
    "pts_boro[\"commercial_STR\"] = pts_boro[\"commercial_STR\"].astype(bool)\n",
    "\n",
    "borough_comm = (\n",
    "    pts_boro.groupby(\"borough\", as_index=False)\n",
    "    .agg(\n",
    "        n_total=(\"listing_id\", \"nunique\"),\n",
    "        n_commercial=(\"commercial_STR\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "borough_comm[\"commercial_share\"] = borough_comm[\"n_commercial\"] / borough_comm[\"n_total\"]\n",
    "\n",
    "boros_comm = boros.merge(borough_comm[[\"borough\", \"commercial_share\"]], on=\"borough\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d339cbc-ffa0-49d3-a546-3b96021d207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.5 Spatial Map 1: STR density per 1,000 dwellings\n",
    "SHOW_MAP_STR_DENSITY = True\n",
    "\n",
    "if SHOW_MAP_STR_DENSITY:\n",
    "    ax = boros_density.plot(\n",
    "        column=\"str_density_per_1000\",\n",
    "        legend=True,\n",
    "        figsize=(8, 8),\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "        missing_kwds={\n",
    "            \"color\": \"lightgrey\",\n",
    "            \"label\": \"No data\"\n",
    "        },\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Map-1 STR Density (Listings per 1,000 dwellings)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32305787-ae4f-4be7-8d25-f78a2e3b8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.6 Spatial Map 2: Commercial STR share by borough\n",
    "SHOW_MAP_COMMERCIAL = True\n",
    "\n",
    "if SHOW_MAP_COMMERCIAL:\n",
    "    ax = boros_comm.plot(\n",
    "        column=\"commercial_share\",\n",
    "        legend=True,\n",
    "        linewidth=0.6,\n",
    "        edgecolor=\"white\",\n",
    "        figsize=(7, 7),\n",
    "    )\n",
    "    ax.set_title(\"Map-2 Commercial STR Share by Borough\")\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521b76c-8400-47e5-85a0-6f06e01a0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Spatial Autocorrelation (Global Moran + Moran scatter + Local Moran LISA map)\n",
    "## 7.1 Defination & Compute\n",
    "# Use the borough-level STR density surface (boros_density) for autocorrelation\n",
    "gdf = boros_density[[\"geometry\", \"str_density_per_1000\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "y = gdf[\"str_density_per_1000\"].to_numpy(dtype=float)\n",
    "z = (y - y.mean()) / y.std(ddof=1)\n",
    "\n",
    "# Queen contiguity weights (Practical 7 style)\n",
    "w = Queen.from_dataframe(gdf, use_index=False)\n",
    "w.transform = \"R\"\n",
    "\n",
    "n = len(z)\n",
    "S0 = w.s0\n",
    "Wz = w.sparse @ z\n",
    "\n",
    "# Global Moran's I\n",
    "I_obs = (n / S0) * (z @ Wz) / (z @ z)\n",
    "print(f\"Global Moran's I (observed): {I_obs:.4f}\")\n",
    "\n",
    "# Permutation test\n",
    "perms = 999\n",
    "rng = np.random.default_rng(42)\n",
    "I_perm = np.empty(perms)\n",
    "\n",
    "for p in range(perms):\n",
    "    z_perm = rng.permutation(z)\n",
    "    Wz_perm = w.sparse @ z_perm\n",
    "    I_perm[p] = (n / S0) * (z_perm @ Wz_perm) / (z_perm @ z_perm)\n",
    "\n",
    "p_two = (np.sum(np.abs(I_perm) >= np.abs(I_obs)) + 1) / (perms + 1)\n",
    "print(f\"Permutation p-value (two-sided, {perms} perms): {p_two:.4f}\")\n",
    "\n",
    "# Local Moran's I (LISA) without esda \n",
    "# prerequisites: gdf, z, w (Queen, row-standardised), lag_z already computed as Wz\n",
    "\n",
    "alpha = 0.05\n",
    "perms = 999\n",
    "seed = 42\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# spatial lag (row-standardised)\n",
    "lag_z = w.sparse @ z\n",
    "\n",
    "# observed local Moran statistic (common simple form)\n",
    "Ii_obs = z * lag_z\n",
    "\n",
    "# precompute neighbor list + weights\n",
    "neighbors = w.neighbors\n",
    "weights = w.weights\n",
    "\n",
    "p_local = np.ones(len(z), dtype=float)\n",
    "\n",
    "# GLOBAL permutation: shuffle entire z each time (less conservative than neighbor-only)\n",
    "# For each i, build distribution of Ii_perm = z_i * sum_j w_ij z_perm_j\n",
    "for i in range(len(z)):\n",
    "    neigh = neighbors[i]\n",
    "    if len(neigh) == 0:\n",
    "        p_local[i] = 1.0\n",
    "        continue\n",
    "\n",
    "    w_i = np.asarray(weights[i], dtype=float)\n",
    "    z_i = z[i]\n",
    "\n",
    "    sims = np.empty(perms, dtype=float)\n",
    "    for p in range(perms):\n",
    "        z_perm = rng.permutation(z)                 # shuffle ALL areas\n",
    "        sims[p] = z_i * np.sum(w_i * z_perm[neigh]) # keep i fixed, permute others\n",
    "\n",
    "    # two-sided p-value (more strict)\n",
    "    p_local[i] = (np.sum(sims >= Ii_obs[i]) + 1) / (perms + 1)\n",
    "\n",
    "# store results\n",
    "gdf_lisa = gdf.copy()\n",
    "gdf_lisa[\"z\"] = z\n",
    "gdf_lisa[\"lag_z\"] = lag_z\n",
    "gdf_lisa[\"p\"] = p_local\n",
    "\n",
    "def lisa_quad(z_i, lag_i, p_i, alpha=0.05):\n",
    "    if p_i > alpha:\n",
    "        return \"Not significant\"\n",
    "    if z_i > 0 and lag_i > 0:\n",
    "        return \"High-High\"\n",
    "    if z_i < 0 and lag_i < 0:\n",
    "        return \"Low-Low\"\n",
    "    if z_i > 0 and lag_i < 0:\n",
    "        return \"High-Low\"\n",
    "    if z_i < 0 and lag_i > 0:\n",
    "        return \"Low-High\"\n",
    "    return \"Not significant\"\n",
    "\n",
    "gdf_lisa[\"cluster\"] = [\n",
    "    lisa_quad(gdf_lisa.loc[i, \"z\"], gdf_lisa.loc[i, \"lag_z\"], gdf_lisa.loc[i, \"p\"], alpha=alpha)\n",
    "    for i in range(len(gdf_lisa))\n",
    "]\n",
    "\n",
    "print(gdf_lisa[\"cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d00fff-82e0-4e43-be0a-80f01760b2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b3e3e-31cc-4e75-ae26-8cabfaf77cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4020c-83a8-4a12-9b2f-6d107b7294ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.2 Global Moran's I: Moran scatterplot\n",
    "SHOW_MORAN_SCATTER = False\n",
    "\n",
    "# Moran scatterplot\n",
    "if SHOW_MORAN_SCATTER:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(z, lag_z, s=25)\n",
    "    plt.axhline(0, linewidth=1)\n",
    "    plt.axvline(0, linewidth=1)\n",
    "\n",
    "    b = np.polyfit(z, lag_z, 1)[0]  # slope ~ Moran's I\n",
    "    xs = np.array([z.min(), z.max()])\n",
    "    plt.plot(xs, b * xs)\n",
    "\n",
    "    plt.xlabel(\"z (STR density)\")\n",
    "    plt.ylabel(\"Spatial lag of z (Wz)\")\n",
    "    plt.title(f\"Moran scatterplot (slope≈I={I_obs:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57419a-84e4-4246-8652-5a4e9c722198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.3 Local Moran's I: LISA Cluster Map\n",
    "SHOW_LISA_MAP = True\n",
    "\n",
    "if SHOW_LISA_MAP:\n",
    "    ax = gdf_lisa.plot(\n",
    "        column=\"cluster\",\n",
    "        categorical=True,\n",
    "        legend=True,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.8,\n",
    "        figsize=(8, 8),\n",
    "    )\n",
    "    ax.set_title(f\"Map-3 Local Moran's I (LISA) clusters for STR density (p ≤ {alpha})\")\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94390f3-0f18-48ab-9ce1-1c7c63ea7b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
